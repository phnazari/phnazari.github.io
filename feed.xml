<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://phnazari.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://phnazari.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-05T14:55:59+00:00</updated><id>https://phnazari.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Dual Complexity Measures for ReLU Networks</title><link href="https://phnazari.github.io/blog/2025/dual-complexity-measures/" rel="alternate" type="text/html" title="Dual Complexity Measures for ReLU Networks"/><published>2025-06-04T00:00:00+00:00</published><updated>2025-06-04T00:00:00+00:00</updated><id>https://phnazari.github.io/blog/2025/dual-complexity-measures</id><content type="html" xml:base="https://phnazari.github.io/blog/2025/dual-complexity-measures/"><![CDATA[<h1 id="dual-complexity-measures">Dual Complexity Measures</h1> <p>This is part two of a three part series on the geometry of generalization, which is the essence of my <a href="/assets/pdf/Master_Thesis.pdf">master thesis</a><d-cite key="nazari2025thesis"></d-cite>. In this series, we will present a novel perspective on generalization of overparameterized networks.</p> <p>The series is structured in the following way:</p> <ul> <li>In <a href="/blog/2025/dual-representation/">part one</a>, we established a dual representation of fully connected feedforward ReLU networks.</li> <li>In <a href="/blog/2025/dual-complexity-measures/">this post</a>, we show how this dual representation can be used to derive complexity measures for these networks.</li> <li>In part three (coming soon!!), we will use these complexity measures to find evidence for the volume hypothesis<d-cite key="chiang2022loss"></d-cite>, an approach to explain why overparameterized models generalize well.</li> </ul> <h2 id="introduction">Introduction</h2> <p>The constructions put forward in this post are inspired by Piwek et al.<d-cite key="piwek2023exact"></d-cite>. In particular, we utilize the dual representation introduced in <a href="/blog/2025/dual-representation/">part one</a> to assess the complexity of ReLU networks.</p> <p>As complexity measures, we use the number of affine regions in the setting of regression (as in <d-cite key="montufar2014number, zhang2018tropical, raghu2017expressive, pascanu2013number"></d-cite>) and the number of linear pieces in the decision boundary in the setting of binary classification (as in <d-cite key="piwek2023exact"></d-cite>). As we will show, these two complexity measures can be directly tied to the dual representation: the number of linear pieces in the decision boundary corresponds to the number of a specific kind of edge in an upper convex hull (<a href="#corollary:nn-boundary-dual" class="cite-stmt hover-link">Corollary</a>), and the number of affine regions corresponds to the number of vertices in another upper convex hull (<a href="#corollary:affine-pieces-mod" class="cite-stmt hover-link">Corollary</a>).</p> <p>Finally, for every claim made in this series, I will refer to the corresponding statement in the thesis. There, one can find a proof as well as a reference to the corresponding statement by Piwek et al.<d-cite key="piwek2023exact"></d-cite> wherever appropriate.</p> <h2 id="tessellations">Tessellations</h2> <p>CPA functions (see <a href="/blog/2025/dual-representation//#affine-and-cpa-functions">this section</a> of the previous post) induce a <em>tessellation</em> of $\mathbb R^d$. It plays an important role in understanding ReLU networks:</p> <div class="statement definition" id="def:tessellation"> <strong>(Tessellations)</strong> Given a CPA function $F(\mathbf x) := \max\{f_1(\mathbf x),\ldots,f_n(\mathbf x)\}$, a <em>cell</em> induced by $F$ is $$ \begin{equation*} \{\mathbf x \in \mathbb R^d \;|\; f_i(\mathbf x) = f_{i'}(\mathbf x) \geq f_j(\mathbf x) \; \text{for all } i,i' \in I, j \in J\}, \end{equation*} $$ where $I, J$ are disjoint sets whose union is $\{1,2,...,n\}$. The set of all cells induced $F$ is called the <em>tessellation</em> induced by $F$ and denoted by $\mathcal T(F)$. </div> <p><a href="#fig:tessellation" class="cite-fig hover-link">Figure</a> contains an example of a tessellation. By a slight abuse of notation, we will write $\mathcal T(S)$ for the tessellation induced by the CPA function $\mathcal Q(S)$ for any set of points $S$.</p> <p>The following lemma establishes a connection between tessellations and polyhedral complexes:</p> <div class="statement lemma"> <strong>(Lemma 3.4.2 in Nazari<d-cite key="nazari2025key"></d-cite>)</strong> The tessellation induced by a CPA function $F$ forms a polyhedral complex. </div> <p>This lemma tells us that we may think of a tessellation as a polyhedral complex and thus the following definition makes sense:</p> <div class="statement definition"> Let $F$ be a CPA function. We denote by $\mathcal T_k(F)$ the $k$-skeleton of the tessellation induced by $F$. The support of $\mathcal T_{d-1}(F)$ is also called an <em>affine (or tropical) hypersurface</em>. % The $d$-cells are called <em>affine regions</em>, since they are the largest connected sets on which $F$ is an affine function. </div> <h3 id="example">Example</h3> <p>As an example, <a href="#fig:tessellation" class="cite-fig hover-link">Figure</a> shows the tessellation induced by the CPA function \begin{equation} \label{eq:example-tessellation} f \colon \mathbb R^2 \to \mathbb R, \quad (x,y) \mapsto \max {1+2x, 1+2y, 2+x+y, 2+x, 2+y, 2}. \end{equation} The blue lines correspond to points on which two affine functions agree and are larger than the others. They form the $1$-skeleton of the tessellation. The intersections of these lines are the $0$-cells. On each of the white convex regions (the $2$-cells) $f$ is affine.</p> <p><a href="#fig:tessellation" class="cite-fig hover-link">Figure</a> also illustrates how the tesselation forms a polyhedral complex: the face of any polyhedron is again a polyhedron (for example, the faces of the white convex regions are the $1$-cells), and the intersection of any two polyhedra is either empty or again a face.</p> <div class="row mt-3 w-50 mx-auto figure-content" id="fig:tessellation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dual_representation/tessellation-480.webp 480w,/assets/img/dual_representation/tessellation-800.webp 800w,/assets/img/dual_representation/tessellation-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dual_representation/tessellation.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption-left figure"> Figure 1 in <d-cite key="zhang2018tropical"></d-cite>. Example of a tessellation, induced by the DCPA-function given in Equation \eqref{eq:example-tessellation}. </div> <h2 id="decision-boundary">Decision Boundary</h2> <p>We saw in <a href="/blog/2025/dual-representation/">the previous post</a> how we can identify neural networks as DCPA functions using affine geometry. In this section, we use this result to characterize the decision boundary of ReLU binary classification networks. This will eventually allow counting the linear pieces inside the decision boundary.</p> <p>Throughout this section, let $S \subseteq \mathcal D$ be a set of dual points whose upper convex hull has vertices \(\mathcal U^*(S) = \{s_1,\ldots,s_n\} = \{(\mathbf a_1,b_1),\ldots,(\mathbf a_n,b_n)\}.\) Furthermore, given a set of indices $I \subseteq {1,\ldots,n}$, we introduce the short-hand notation \(S_I := \{s_i \mid i \in I\}\) for the subset of $S$ indexed by $I$.</p> <h3 id="characterizing-k-cells">Characterizing $k$-cells</h3> <p>Let $\sigma \in \mathcal T(S)$ be a cell in the tessellation induced by $S$. Using the definition and the fact that CPA functions are uniquely characterized by their upper convex hulls (see see <a href="/blog/2025/dual-representation//#cpa-functions-as-upper-convex-hulls">this section</a> of the previous post), one can quickly confirm that $\sigma$ is the solution of a system of linear inequalities and equalities:</p> \[\begin{align} \label{eq:system-zero} \left\{ \begin{aligned} \mathcal R(s_i)(\mathbf x) &amp;= \mathcal R(s_{i'})(\mathbf x) \quad \forall\, i,i' \in I^{\sigma}_= \\ \mathcal R(s_i)(\mathbf x) &amp;\ge \mathcal R(s_j)(\mathbf x) \quad \forall\, i \in I_=^{\sigma},\, j \in I^{\sigma}_+, \end{aligned} \right. \end{align}\] <div> where $I^{\sigma}_{=}$ and $I^{\sigma}_{+}$ form a disjoint partition of $\{1,2,\ldots,n\}$. W.l.o.g., this partition can be chosen in such a way that no index can be moved from $I^{\sigma}_+$ to $I^{\sigma}_=$ without altering the solution space. </div> <p>Using that $\mathcal R(s_i)(\mathbf x) := \langle \mathbf a_i, \mathbf x \rangle + b_i$ for all $s_i = (\mathbf a_i, b_i) \in S$, System \eqref{eq:system-zero} can be re-written as a system of linear inequalities:</p> <div class="statement definition" id="def:cells-as-systems"> <strong>(Cells as System of Linear Inequalities)</strong> Any cell $\sigma \in \mathcal T(S)$ can be written as a system of linear inequalities and equalities $\sigma = \{\mathbf A^{\sigma}_= \mathbf x = \mathbf b^{\sigma}_=\} \cap \{\mathbf A^{\sigma}_+ \mathbf x \ge \mathbf b^{\sigma}_+\}$ in the following way. Fix a dual point $s_{k_{\sigma}} \in S_{I^{\sigma}_=}$. The matrix $\mathbf A^{\sigma}_= \in \mathbb R^{|I^{\sigma}_=| \times d}$ containing the equality constraints has as its rows the vectors $\bigl(\mathbf a_{k_{\sigma}} - \mathbf a_i \mid i \in I^{\sigma}_=\bigr)$ and the corresponding vector $\mathbf b^{\sigma}_= \in \mathbb R^{|I^{\sigma}_=|}$ has entries $\bigl(b_i - b_{k_{\sigma}} \mid i \in I^{\sigma}_=\bigr)$ (in the same order). Similarly, the matrix $\mathbf A^{\sigma}_+ \in \mathbb R^{|I^{\sigma}_+| \times d}$ containing the inequality constraints has as its rows the vectors $\bigl(\mathbf a_{k_{\sigma}} - \mathbf a_i \mid i \in I^{\sigma}_+\bigr)$ and the corresponding vector $\mathbf b^{\sigma}_+ \in \mathbb R^{|I^{\sigma}_+|}$ has entries $\bigl(b_i - b_{k_{\sigma}} \mid i \in I^{\sigma}_+\bigr)$. </div> <div class="statement remark" id="remark:cells-as-systems"> <strong>(Remark 6.1.2 in Nazari<d-cite key="nazari2025thesis"></d-cite>)</strong> The joint system of linear equalities and linear inequalities in <a href="#def:cells-as-systems" class="cite-stmt hover-link">Definition</a> can be translated to a system of just inequalities $$ \sigma = \{\mathbf A^{\sigma} \mathbf x \ge \mathbf b^{\sigma}\} = \{\mathbf A^{\sigma}_= \mathbf x = \mathbf b^{\sigma}_=\} \cap \{\mathbf A^{\sigma}_+ \mathbf x \ge \mathbf b^{\sigma}_+\} $$ by rewriting every equality as two inequalities. The resulting matrix $\mathbf A^{\sigma} \in \mathbb R^{2|I_=^{\sigma}| \times d}$ contains the rows of $\mathbf A^{\sigma}_=$, as well as their negatives, and the rows of $\mathbf A^{\sigma}_+$. The vector $\mathbf b^{\sigma} \in \mathbb R^{2|I_=^{\sigma}|}$ can be constructed analogously. </div> <p>Using this representation of $\sigma$ as a system of linear inequalities, the following proposition describes the dimension of $\sigma$:</p> <div class="statement proposition" id="proposition:dim-cell-rank"> <strong>(Proposition 6.1.3 in Nazari<d-cite key="nazari2025thesis"></d-cite>)</strong> Let $\sigma \in \mathcal T(S)$ be a cell in the tessellation induced by $S$. Then $$ \dim \sigma \;=\; d \;-\; \mathrm{rank}\,\mathbf A^{\sigma}_=. $$ </div> <p>As a next step, we aim to understand how the cell $\sigma$ looks in dual space. We start with the following proposition:</p> <div class="statement proposition" id="proposition:dim-face-rank"> <strong>(Proposition 6.1.4 in Nazari<d-cite key="nazari2025thesis"></d-cite>)</strong> Let $\sigma \in \mathcal T(S)$ be a cell. Then the convex hull $\mathcal C(S_{I_=^{\sigma}})$ is a face in $\mathcal U(S)$ of dimension $$ \dim \mathcal C\bigl(S_{I_=^{\sigma}}\bigr) \;=\; \mathrm{rank}\,\mathbf A_=^{\sigma}. $$ </div> <p>The last proposition tells us that $\mathcal C(S_{I_=^{\sigma}})$ is a face in $\mathcal U(S)$. The next proposition uses the face $\mathcal C(S_{I_=^{\sigma}})$ to explain how $\sigma$ translates to dual space. But first, we define what it means for an affine function to be tangent to an upper convex hull:</p> <div class="statement definition" id="def:tangent-to-uch"> <strong>(Tangent Affine Function)</strong> Given an affine function $f \colon \mathbb R^d \to \mathbb R$, we say that $f$ is <em>tangent</em> to the upper convex hull $\mathcal U(S)$ if <ol style="i"> <li>$f$ lies above $\mathcal U(S)$, i.e., $f \succeq \mathcal U(S)$.</li> <li>The graph of $f$ intersects the upper convex hull, i.e., $\mathrm{graph}(f) \cap \mathcal U(S) \neq \emptyset$.</li> </ol> In this case, we write $f \;\|\; \mathcal U(S)$. </div> <div class="statement proposition" id="proposition:one-cell-one-plane"> <strong>(Proposition 6.1.6 in Nazari<d-cite key="nazari2025thesis"></d-cite>)</strong> Let $\sigma \in \mathcal T(S)$ be a cell in the tessellation induced by $S$. Then there is a one-to-one correspondence between points in $\sigma$ and dual planes tangent to the upper convex hull of $S$ which contain the face $\mathcal C(S_{I_=^{\sigma}})$: $$ \{\mathbf x \in \sigma\} \;\longleftrightarrow\; \{\,f \in \text{Aff}_{\mathfrak D}(d) \mid f \| \mathcal U(S) \text{ and } f \supseteq \mathcal C(S_{I_=^{\sigma}})\}. $$ Similarly, every face in $\mathcal U(S)$ defines a cell in this way. </div> <p>By <a href="#proposition:dim-face-rank" class="cite-stmt hover-link">Proposition</a> and <a href="#proposition:one-cell-one-plane" class="cite-stmt hover-link">Proposition</a>, cells in the tessellation induced by $S$ are closely related to faces in the upper convex hull of $S$. The following theorem makes this relationship precise:</p> <div class="statement theorem" id="theorem:cell-face-bijection"> <strong>(Theorem 6.1.7 in Nazari<d-cite key="nazari2025thesis"></d-cite>)</strong> There exists a one-to-one correspondence between $k$-cells in $\mathcal T(S)$ and $(d-k)$-faces in $\mathcal U(S)$. Specifically, the following map is a bijection: $$ \begin{aligned} \Phi \colon \mathcal T_k(S) &amp;\;\xrightarrow{\sim}\; \mathcal U_{\,d-k}(S) \\ \sigma &amp;\;\mapsto\; \mathcal C\bigl(S_{I_=^{\sigma}}\bigr). \end{aligned} $$ </div> <h3 id="application-to-the-decision-boundary">Application to the Decision Boundary</h3> <p>In this section, we use the bijection from <a href="#theorem:cell-face-bijection" class="cite-stmt hover-link">Theorem</a> to characterize the decision boundary of a ReLU binary classification network $\mathcal N = \mathcal Q(P) - \mathcal Q(N) \colon \mathbb R^d \to \mathbb R$.</p> <p>As a quick reminder, the network’s decision boundary is given by \(\mathcal B \;=\; \bigl(\mathcal Q(P) - \mathcal Q(N)\bigr)^{-1}(0).\) Consequently, we are interested in studying zero-sets of DCPA functions. We start with a special case.</p> <div class="statement proposition" id="proposition:linear-pieces-special"> <strong>(Decision Boundary I, Proposition 6.2.1 in Nazari<d-cite key="nazari2025thesis"></d-cite>)</strong> Let $F = \mathcal Q(P)$ and $G = \mathcal Q(N)$ be CPA functions $\mathbb R^d \to \mathbb R$ for some finite sets of dual points $P, N \subseteq \mathcal D$. Assume that no point of $P$ lies on $\mathcal U(N)$ and vice versa. Let $D$ be the zero-set of $F - G$. Then $D$ is the union of precisely those $(d-1)$-cells of $\mathcal T(P \cup N)$ which (in the sense of <a href="#theorem:cell-face-bijection" class="cite-stmt hover-link">Theorem</a>) correspond to edges (i.e., $1$-faces) of $\mathcal U(P \cup N)$ with one end in $P$ and the other end in $N$. </div> <p><a href="#proposition:linear-pieces-special" class="cite-stmt hover-link">Proposition</a> handles the special case that $P \cap \mathcal U(N) = N \cap \mathcal U(P) = \emptyset$. The following proposition handles the general case.</p> <div class="statement proposition" id="proposition:linear-pieces-general"> <strong>(Decision Boundary II, Proposition 6.2.2 in Nazari<d-cite key="nazari2025thesis"></d-cite>)</strong> Let $F = \mathcal Q(P)$ and $G = \mathcal Q(N)$ be CPA functions $\mathbb R^d \to \mathbb R$ for some finite sets of dual points $P, N \subseteq \mathcal D$. Let $D$ be the zero-set of $F - G$. Then $D$ is the union of precisely those $(d-1)$-cells of $\mathcal T(P \cup N)$ which (in the sense of <a href="#theorem:cell-face-bijection" class="cite-stmt hover-link">Theorem</a>) correspond to edges of $\mathcal U(P \cup N)$ containing points from both $P$ and $N$. </div> <p>For completeness, the following corollary applies these findings to neural networks.</p> <div class="statement corollary" id="corollary:nn-boundary-dual"> <strong>(Corollary 6.2.3 in Nazari<d-cite key="nazari2025thesis"></d-cite>)</strong> Let $\mathcal Q(P) - \mathcal Q(N) \colon \mathbb R^d \to \mathbb R$ be a ReLU binary classification network. Then the number of linear pieces in the decision boundary of $\mathcal N$ equals the number of edges in $\mathcal U(P \cup N)$ containing points from both $P$ and $N$. </div> <div class="row mt-3 w-50 mx-auto figure-content" id="fig:example-surfaces"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dual_representation/example-dual-rep.svg" sizes="95vw"/> <img src="/assets/img/dual_representation/example-dual-rep.svg" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption-left figure"> An example of points $(P,N)$ (points in $N$ are red, points in $P$ are blue), defining a ReLU network $\mathcal N = \mathcal Q(P) - \mathcal Q(N) \colon \mathbb R^2 \to \mathbb R$. There are four edges (light blue) contributing to the decision boundary of $\mathcal N$, since they contain both red and blue points. However, only three of them start and end in different colors. </div> <p>Some thoughts on the difference between <a href="#proposition:linear-pieces-special" class="cite-stmt hover-link">Proposition</a> and <a href="#proposition:linear-pieces-general" class="cite-stmt hover-link">Proposition</a> can be found in Remark 6.2.4 in Nazari<d-cite key="nazari2025thesis"></d-cite>.</p> <h3 id="example-1">Example</h3> <p>In this subsection, we continue the toy-example from <a href="/blog/2025/dual-representation//#example">the previous post</a> (see Example 5.2.6 in Nazari<d-cite key="nazari2025thesis"></d-cite>). By <a href="#corollary:nn-boundary-dual" class="cite-stmt hover-link">Corollary</a>, the number of linear pieces in its decision boundary is the same as the number of edges in $\mathcal U(P \cup N)$ containing points from both $P$ and $N$.</p> <p>Specifically, one can compute \(\mathcal U^*(P \cup N) = \{(5,19,5),\,(0,14,7),\,(12,5,-2)\}\)</p> <div> where $(5,19,5),(12,5,-2) \in \mathcal U^*(P)$ and $(0,14,7) \in \mathcal U^*(N)$ (see <a href="#fig:tropical-toy-example-union" class="cite-fig hover-link">Figure</a>). Thus, there are three edges in $\mathcal U(P \cup N)$, two of which contribute to the network’s decision boundary since they contain vertices from both $P$ and $N$. This confirms Figure <em>see caption below</em>, which shows the decision boundary of $\mathcal N$ and confirms that, indeed, it consists of two linear pieces. </div> <div class="row mt-3 w-50 mx-auto figure-content" id="fig:tropical-toy-example-union"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dual_representation/running-example-uch.svg" sizes="95vw"/> <img src="/assets/img/dual_representation/running-example-uch.svg" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption-left figure"> Two-dimensional toy-example defined in <a href="/blog/2025/dual-representation/">part one</a>. Red points correspond to $N$, blue points are $P$. The green polygon is $\mathcal U(P \cup N)$. Note that, in theory, $\mathcal U(P \cup N)$ and is a polyhedral complex, i.e., it can consist of multiple facets. Note also how there are red and blue points in $\mathcal U^*(P \cup N)$, ultimately contributing to the decision boundary. </div> <h2 id="affine-regions">Affine Regions</h2> <p>In the previous section, we used the upper convex hull of $P \cup N$ to characterize the decision boundary of a ReLU binary classification network $\mathcal Q(P) - \mathcal Q(N)$. In this section, we take a similar approach to characterize the network’s affine regions.</p> <h3 id="refinements">Refinements</h3> <p>So far, we have studied what it means for a CPA function to induce a tessellation of $\mathbb R^d$. The following definition clarifies what it means for a DCPA function to do so.</p> <div class="statement definition" id="def:tessellation-refinement"> <strong>(Tessellation Induced by a DCPA Function)</strong> Let $F = \mathcal Q(P) - \mathcal Q(N)$ be a DCPA function. We then define the tessellation $\mathcal T(P,N)$ induced by $F$ to consist of all non-empty pairwise intersections of cells induced by $P$ and $N$, i.e. $$ \mathcal T(P, N) := \{\sigma \cap \sigma' \mid \sigma \in \mathcal T(P),\; \sigma' \in \mathcal T(N)\} \setminus \emptyset. $$ </div> <p>As it turns out, $\mathcal T(P,N)$ is closely related to tessellations induced by different CPA functions:</p> <div class="statement definition"> <strong>(Refinements)</strong> Let $\mathcal T$ and $\mathcal F$ be tessellations of $\mathbb R^d$. We say that $\mathcal T$ is a <em>refinement</em> of $\mathcal F$ if every cell of $\mathcal T$ is contained in a cell of $\mathcal F$. In this case, we write $\mathcal T \ll \mathcal F$. </div> <div class="statement lemma" id="lemma:refinement"> <strong>(Lemma 3.4.7 in Nazari<d-cite key="nazari2025thesis"></d-cite>)</strong> Given two sets of dual points $P, N \subseteq \mathcal D$, it holds that $$ \mathcal T(P \cup N) \ll \mathcal T(P, N) \ll \mathcal T(N). $$ </div> <h3 id="characterizing-k-cells-part-ii">Characterizing $k$-cells, Part II</h3> <p>Any cell $\sigma \in \mathcal{T}(P, N)$ is of the form $\sigma = \sigma’\cap \sigma’’$ for some $\sigma’ \in \mathcal{T}(P)$ and $\sigma’’ \in \mathcal{T}(N)$. In <a href="#def:cells-as-systems" class="cite-stmt hover-link">Definition</a> and <a href="#remark:cells-as-systems" class="cite-stmt hover-link">Remark</a>, we saw how $\sigma’$ and $\sigma’’$ can be expressed as the solution of a system of linear inequalities $\sigma’ = {\mathbf A^{\sigma’} \mathbf x \ge \mathbf b^{\sigma’}}$, $\sigma’’ = {\mathbf A^{\sigma’’} \mathbf x \ge \mathbf b^{\sigma’’}}$. This induces a similar representation for $\sigma$:</p> \[\begin{equation} \sigma = \left\{ \begin{bmatrix} \mathbf A^{\sigma'} \\ \mathbf A^{\sigma''} \end{bmatrix} \mathbf x \;\ge\; \begin{bmatrix} \mathbf b^{\sigma'} \\ \mathbf b^{\sigma''} \end{bmatrix} \right\}. \end{equation}\] <p>Analogously to the previous section, we now turn our attention to the induced system of implicit equalities.</p> <div class="statement definition"> <strong>(Refined Cells as System of Linear Inequalities)</strong> Let $\{\mathbf A_=^{\sigma',\sigma''} \mathbf x = \mathbf b_=^{\sigma',\sigma''}\}$ be the system of implicit equalities in $\sigma$ coming from $\sigma'$. That is, any row $\mathbf a_i^{\sigma'} \in \mathbf A_=^{\sigma',\sigma''}$ is also a row in $\mathbf A^{\sigma'}$ and satisfies $$ \langle \mathbf a_i^{\sigma'}, \mathbf x \rangle = b_i^{\sigma'} \quad \forall\, \mathbf x \in \sigma' \cap \sigma''. $$ We write $I_=^{\sigma',\sigma''}$ for the set indexing these implicit equality constraints. Generally, $I_=^{\sigma'} \subseteq I_=^{\sigma',\sigma''}$, since the latter could contain constraints that only become implicit equalities in combination with $\sigma''$ (see <a href="#fig:two-polytopes-implicit" class="cite-fig hover-link">Figure</a> for an example). Similarly, define the system of implicit equalities in $\sigma$ coming from $\sigma''$ as $\{\mathbf A_=^{\sigma'',\sigma'} \mathbf x = \mathbf b_=^{\sigma'',\sigma'}\}$. That is, any row $\mathbf a_i^{\sigma''} \in \mathbf A_=^{\sigma'',\sigma'}$ is also a row in $\mathbf A^{\sigma''}$ and satisfies $$ \langle \mathbf a_i^{\sigma''}, \mathbf x \rangle = b_i^{\sigma''} \quad \forall\, \mathbf x \in \sigma' \cap \sigma''. $$ Again, let $I_=^{\sigma'',\sigma'}$ be the set indexing these implicit equality constraints. </div> <div class="row mt-3 w-50 mx-auto figure-content" id="fig:two-polytopes-implicit"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dual_representation/green-line.svg" sizes="95vw"/> <img src="/assets/img/dual_representation/green-line.svg" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption-left figure"> Example of a cell $\sigma$ (green line) in the tessellation $\mathcal{T}(P, N)$, formed by the intersection of a cell ${\sigma}^{\prime} \in \mathcal{T}(P)$ (blue) and a cell ${\sigma}^{\prime\prime} \in \mathcal{T}(N)$ (red). Both ${\sigma}^{\prime}$ and ${\sigma}^{\prime\prime}$ have co-dimension one, each satisfying a single equality constraint. Their intersection imposes an additional equality constraint, resulting in $\sigma$ having co-dimension $2$. </div> <p>The following proposition describes the dimension of $\sigma$ in this setup:</p> <div class="statement proposition" id="proposition:dim-cell-refined"> <strong>(Proposition 7.0.2 in Nazari<d-cite key="nazari2025thesis"></d-cite>)</strong> Let $\sigma = \sigma' \cap \sigma'' \in \mathcal T(P,N)$ be a cell in the tessellation induced by $\mathcal Q(P) - \mathcal Q(N)$. Then $$ \dim \sigma \;=\; d \;-\; \mathrm{rank}\! \begin{bmatrix} \mathbf A_=^{\sigma',\,\sigma''} \\ \mathbf A_=^{\sigma'',\,\sigma'} \end{bmatrix}. $$ </div> <p>Like derived for the decision boundary in the previous section, the next step is to understand how $\sigma$ appears in dual space. We begin with the following proposition:</p> <div class="statement proposition" id="proposition:cell-face-cont-2"> <strong>(Proposition 7.0.3 in Nazari<d-cite key="nazari2025thesis"></d-cite>)</strong> Let $\sigma = \sigma' \cap \sigma'' \in \mathcal T_k(P,N)$ be a $k$-cell in the tessellation induced by $\mathcal Q(P) - \mathcal Q(N)$. Then $$ \mathcal C\bigl(P_{\,I_=^{\sigma',\,\sigma''}} \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N_{\,I_=^{\sigma'',\,\sigma'}}\bigr) \;\in\; \mathcal U_{\,d-k}(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N). $$ </div> <p>The last proposition tells us that $\mathcal C\bigl(P_{\,I_=^{\sigma’,\,\sigma’’}} \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N_{\,I_=^{\sigma’’,\,\sigma’}}\bigr)$ is a face in $\mathcal U(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)$. As in the previous section, the next proposition uses this face to explain how $\sigma$ looks in dual space:</p> <div class="statement proposition"> <strong>(Proposition 7.0.4 in Nazari<d-cite key="nazari2025thesis"></d-cite>)</strong> Let $\sigma = \sigma' \cap \sigma'' \in \mathcal T(P, N)$ be a cell in the tessellation induced by $\mathcal Q(P) - \mathcal Q(N)$. Then there is a one-to-one correspondence between points in $\sigma$ and dual planes tangent to the upper convex hull $\mathcal U(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)$ containing the face \[ \mathcal C\bigl(P_{\,I_=^{\sigma',\,\sigma''}} \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N_{\,I_=^{\sigma'',\,\sigma'}}\bigr): \] \[ \{\mathbf x \in \sigma\} \;\longleftrightarrow\; \{\,f \in \text{Aff}_{\mathfrak D}(d) \mid f \| \mathcal U(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N) \text{ and } f \supseteq \mathcal C(P_{\,I_=^{\sigma',\,\sigma''}} \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N_{\,I_=^{\sigma'',\,\sigma'}})\}. \] Similarly, every face in $\mathcal U(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)$ corresponds to a cell in this way. </div> <p>The following theorem makes precise the relationship between cells and faces that was introduced in the previous two propositions:</p> <div class="statement theorem" id="theorem:cell-face-bijection-2"> <strong>(Theorem 7.0.5 in Nazari<d-cite key="nazari2025thesis"></d-cite>)</strong> There exists a one-to-one correspondence between $k$-cells in $\mathcal T(P,N)$ and $(d-k)$-faces in $\mathcal U(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)$. Specifically, the following map is a bijection: $$ \begin{aligned} \Psi \colon \mathcal T_k(P, N) &amp;\xrightarrow{\sim} \mathcal U_{\,d-k}(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N) \\[6pt] \sigma = \sigma' \cap \sigma'' \; &amp;\mapsto \; \mathcal C\bigl(P_{\,I_=^{\sigma',\,\sigma''}} \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N_{\,I_=^{\sigma'',\,\sigma'}}\bigr). \end{aligned} $$ </div> <h3 id="counting-affine-regions">Counting Affine Regions</h3> <p>In the special case where $\dim \sigma = d$, the result in <a href="#theorem:cell-face-bijection-2" class="cite-stmt hover-link">Theorem</a> allows counting the number of affine regions defined by a ReLU network. Indeed, we will show in this section how the affine regions can be constructed from $\mathcal T_d(P,N)$ as a set of equivalence classes. We will then translate this observation to dual space.</p> <p>Let $\sigma \in \mathcal T_d(P,N)$ be a $d$-cell in the tessellation induced by $\mathcal N := \mathcal Q(P) - \mathcal Q(N)$. Then $\mathcal N$ is an affine map when restricted to $\sigma$. In particular, there exist $p^{\sigma} \in P$ and $n^{\sigma} \in N$ such that</p> \[\begin{equation} \label{eq:fbmx-aff} \mathcal N(\mathbf x) \;=\; \bigl(\mathcal R(p^{\sigma}) - \mathcal R(n^{\sigma})\bigr)(\mathbf x) \;=\; \mathcal R(p^{\sigma} - n^{\sigma}) \quad \forall\, \mathbf x \in \sigma. \end{equation}\] <p>However, two $d$-cells can define the same affine map:</p> <div class="statement proposition" id="proposition:same-aff-in-dual"> <strong>(Proposition 7.1.1 in Nazari<d-cite key="nazari2025thesis"></d-cite>)</strong> Let $\sigma, \sigma' \in \mathcal T_d(P,N)$ be two distinct $d$-cells. Then $\sigma$ and $\sigma'$ define the same affine map if and only if the corresponding vertices $\Psi(\sigma) = p^{\sigma} + n^{\sigma}$, $\Psi(\sigma') = p^{\sigma'} + n^{\sigma'} \in \mathcal U^*(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)$ satisfy $$ p^{\sigma} - n^{\sigma} \;=\; p^{\sigma'} - n^{\sigma'}. $$ </div> <p>If two neighboring $d$-cells, i.e., two $d$-cells which share a $(d-1)$-face, define the same affine map, they are part of the same affine region. This implies that affine regions are coarser than $\mathcal T_d(P,N)$. The rest of this section makes this observation more precise and translates it to dual space.</p> <div class="statement definition"> <strong>(Adjacency)</strong> We make the following two definitions: <ol style="i"> <li>We say that two $d$-cells in $\mathcal T_d(P,N)$ are <em>adjacent</em> if they share a $(d-1)$-face.</li> <li>We say two vertices $p_1 + n_1,\,p_2 + n_2 \in \mathcal U^*(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)$ are <em>adjacent</em> if there exists an edge $\tau \in \mathcal U_1(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)$ going from $p_1 + n_1$ to $p_2 + n_2$.</li> </ol> </div> <p>The following proposition relates these two notions of adjacency:</p> <div class="statement proposition" id="proposition:adjacency-in-dual"> <strong>(Proposition 7.1.3 in Nazari<d-cite key="nazari2025thesis"></d-cite>)</strong> Let $\sigma_1, \sigma_2 \in \mathcal T_d(P,N)$ be two distinct $d$-cells. Then $\sigma_1$ and $\sigma_2$ are adjacent if and only if the corresponding vertices $\Psi(\sigma_1) = p_1 + n_1$, $\Psi(\sigma_2) = p_2 + n_2 \in \mathcal U^*(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)$ are adjacent (that is, satisfy $p_1 - n_1 = p_2 - n_2$). </div> <p>We can now construct the equivalence relation which will identify adjacent $d$-cells in $\mathcal T_d(P,N)$ defining the same affine function:</p> <div class="statement definition" id="def:path-of-cells"> <strong>(Path of $d$-cells)</strong> A <em>path of $d$-cells</em> is a sequence $(\sigma_1,\ldots,\sigma_n) \subseteq \mathcal T_d(P,N)$ of $d$-cells such that <ol style="i"> <li>$\mathcal Q(P) - \mathcal Q(N)$ defines the same affine map on $\sigma_i$ and $\sigma_{i+1}$ for all $i=1,\ldots,n-1$.</li> <li>$\sigma_i$ is adjacent to $\sigma_{i+1}$ for all $i=1,\ldots,n-1$.</li> </ol> We write $\mathfrak P(P, N)$ for the set of all paths of $d$-cells in $\mathcal T_d(P,N)$. </div> <div class="statement definition"> <strong>(Equivalence of $d$-cells)</strong> Given two $d$-cells $\sigma, \sigma' \in \mathcal T_d(P,N)$, we write $\sigma \sim \sigma'$ if there exists a path of $d$-cells from $\sigma$ to $\sigma'$. </div> <p>Clearly, $\sim$ defines an equivalence relation.</p> <p>By <a href="#proposition:adjacency-in-dual" class="cite-stmt hover-link">Proposition</a> and <a href="#proposition:same-aff-in-dual" class="cite-stmt hover-link">Proposition</a>, this equivalence relation translates to dual space. This motivates the following definition:</p> <div class="statement definition" id="def:path-of-dual-points"> <strong>(Path of dual points)</strong> A <em>path of dual points</em> is a sequence $\bigl(p^{\sigma_1} + n^{\sigma_1},\ldots,p^{\sigma_n} + n^{\sigma_n}\bigr) \subseteq \mathcal U^*(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)$ of dual points such that <ol style="i"> <li>$p^{\sigma_i} + n^{\sigma_i}$ is adjacent to $p^{\sigma_{i+1}} + n^{\sigma_{i+1}}$ for all $i=1,\ldots,n-1$.</li> <li>$p^{\sigma_i} - n^{\sigma_i} = p^{\sigma_{i+1}} - n^{\sigma_{i+1}}$ for all $i=1,\ldots,n-1$.</li> </ol> We write $\mathfrak P(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)$ for the set of all paths of dual points in $\mathcal U^*(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)$. </div> <p>In particular, this definition induces an equivalence relation $\sim$ on $\mathcal U^*(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)$, where $p_1 + n_1 \sim p_2 + n_2$ if and only if there exists a path of dual points from $p_1 + n_1$ to $p_2 + n_2$.</p> <div class="row mt-3" id="fig:path-correspondence"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dual_representation/path-of-d-cells.svg" sizes="95vw"/> <img src="/assets/img/dual_representation/path-of-d-cells.svg" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"><strong>(a)</strong></figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dual_representation/path_of_dual_points.svg" sizes="95vw"/> <img src="/assets/img/dual_representation/path_of_dual_points.svg" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"><strong>(b)</strong></figcaption> </figure> </div> </div> <div class="caption-left figure"> Example for the one-to-one correspondence between paths of $d$-cells in $\mathcal{T}_d(P,N)$ and paths of dual points in $\mathcal{U}^*(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)$ (<a href="#proposition:path-correspondence" class="cite-stmt hover-link">Proposition</a>). Subfigure <strong>(a)</strong> shows a path of $d$-cells $(\sigma_1,\sigma_2,\sigma_3)$ (blue), consisting of adjacent cells that define the same affine function. In dual space (Subfigure <strong>(b)</strong>), this corresponds to a path (thick, blue) of adjacent vertices $p^{\sigma_i} + n^{\sigma_i}$ with the property that $p^{\sigma_i} - n^{\sigma_i} = p^{\sigma_j} - n^{\sigma_j}$ for all $i,j=1,2,3$. </div> <p>The following proposition relates paths in $\mathcal T_d(P,N)$ to paths in $\mathcal U^*(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)$:</p> <div class="statement proposition" id="proposition:path-correspondence"> <strong>(Proposition 7.1.7 in Nazari<d-cite key="nazari2025thesis"></d-cite>)</strong> There exists a one-to-one correspondence between paths of $d$-cells in $\mathcal T_d(P,N)$ and paths of dual points in $\mathcal U^*(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)$. It is given by $$ \begin{aligned} \Theta \colon \mathfrak P(P,N) &amp;\;\to\; \mathfrak P(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N) \\[4pt] (\sigma_1,\ldots,\sigma_n) &amp;\;\mapsto\; (\,p^{\sigma_1} + n^{\sigma_1},\ldots,p^{\sigma_n} + n^{\sigma_n}\,). \end{aligned} $$ </div> <p>We finally make precise the one-to-one correspondence between affine regions and equivalence classes of $d$-cells in $\mathcal T_d(P,N)$:</p> <div class="statement corollary" id="corollary:affine-pieces-mod"> <strong>(Corollary 7.1.8 in Nazari<d-cite key="nazari2025thesis"></d-cite>)</strong> There exists a one-to-one correspondence between affine regions of a ReLU network $\mathcal Q(P) - \mathcal Q(N) \colon \mathbb R^d \to \mathbb R$ and equivalence classes in $\mathcal U^*(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)/\sim$. </div> <div> To better understand the space $\mathcal U^*(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)/\sim$, define an unweighted graph $G = (V, E)$ with vertices $V := \mathcal U^*(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N) \subseteq \mathbb R^{d+1}$ and edges $E := \mathcal U_1(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)$. If $d=2$, then clearly $G$ is planar. The set $\mathcal U^*(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)/\sim$ arises from the graph $G$ by contracting exactly the paths in $\mathfrak P(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)$ (see <a href="#fig:graph-with-path" class="cite-fig hover-link">Figure</a>). </div> <h3 id="in-practice">In Practice</h3> <p>We have seen above that $d$-cells in $\mathcal T(P,N)$ can be finer than the affine regions of $\mathcal N = \mathcal Q(P) - \mathcal Q(N)$. Since also the activation regions~<d-cite key="hanin2019deep"></d-cite> are generally finer than the affine regions~<d-cite key="hanin2019deep"></d-cite>, one might ask whether the $d$-cells in $\mathcal{T}(P, N)$ are the same as the activation regions. However, this is not the case. As noted by Hanin and Rolnick~<d-cite key="hanin2019deep"></d-cite>, zeroing out a subnetwork may lead to different activation patterns that coalesce into a single linear region.</p> <p>Importantly, the zeroed-out subnetwork does not affect the upper convex hulls of $P$ and $N$ and therefore does not influence the tessellation $\mathcal T(P, N)$. That is, two activation patterns that only differ in the zeroed-out subnetwork do not influence $\mathcal{T}(P, N)$.</p> <p>Therefore, if $\mathcal N$ restricts to the same affine map on two adjacent cells $\sigma_1, \sigma_2 \in \mathcal T_d(P,N)$, two adjacent activation regions which do not just differ by a zeroed-out subnetwork need to coalesce into the same affine region. We conjecture that this happens with probability zero.</p> <p>By the above argument, this means that the corresponding points $p_1 + n_1$ and $p_2 + n_2$ in dual space lie on the upper convex hull and satisfy $p_1 - n_1 = p_2 - n_2$. This furthermore motivates the conjecture, as this seems to be unlikely.</p> <p>These considerationd lead us to conjecture that, in networks with random parameters, the $d$-cells are almost surely the same as the affine regions:</p> <div class="statement conjecture" id="conjecture:counting-affine-pieces"> <strong>(Conjecture 7.1.10 in Nazari<d-cite key="nazari2025thesis"></d-cite>)</strong> The number of affine regions of a random ReLU network $\mathcal Q(P) - \mathcal Q(N)$ is almost surely equal to the number of points in $\mathcal U^*(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)$. </div> <div class="row mt-3 w-50 mx-auto figure-content" id="fig:graph-with-path"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dual_representation/graph.svg" sizes="95vw"/> <img src="/assets/img/dual_representation/graph.svg" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption-left figure"> An example of the graph $G$ induced by the $1$-skeleton $\mathcal U_1(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)$ for input-dimension $d=2$. By <a href="#corollary:affine-pieces-mod" class="cite-stmt hover-link">Corollary</a>, the affine regions induced by $\mathcal Q(P) - \mathcal Q(N)$ correspond to the vertices in the (multi)-graph $G'$ obtained from $G$ by identifying all the vertices along paths $P \in \mathfrak P(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)$ (red). </div> <h3 id="example-2">Example</h3> <p>In this subsection, we continue the example of the toy network. By <a href="#corollary:affine-pieces-mod" class="cite-stmt hover-link">Corollary</a>, the number of affine regions defined by $\mathcal N$ corresponds to the number of vertices in $\mathcal U^*(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)/\sim$.</p> <p>Specifically, we compute</p> \[\begin{align*} P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N &amp;= \{\,(7, 35, 10), (12, 34, 0), (4, 32, 8), (12, 34, 3), (3, 28, 11), (17, 24, 0), (13, 23, 2), \\ &amp;\phantom{=} (11, 36, 1), (11, 36, 4), (14, 21, 3), (2, 30, 12), (16, 26, 1), (16, 26, -1), (10, 38, 7), \\ &amp;\phantom{=} (10, 38, 4), (12, 19, 5),(6, 31, 8), (10, 38, 5), (6, 31, 11), (8, 33, 4), (11, 21, 6), \\ &amp;\phantom{=} (8, 33, 7), (4, 32, 10), (8, 36, 6), (5, 33, 9), (6, 28, 6), (8, 36, 9), (13, 23, 4), (11, 36, 3), \\ &amp;\phantom{=} (18, 22, -3), (5, 33, 12), (11, 36, 6), (5, 30, 7), (15, 22, 2), (7, 35, 5), (9, 34, 5), \\ &amp;\phantom{=} (7, 35, 8), (9, 34, 8), (8, 33, 6), (14, 24, 3), (15, 19, 0), (8, 33, 9), (9, 31, 3), \\ &amp;\phantom{=} (17, 24, -2), (9, 31, 6), (14, 21, 1), (10, 38, 2), (5, 30, 9), (7, 35, 7)\,\} \end{align*}\] <p>and \(\begin{align*} \mathcal U^*(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N) &amp;= \{\,(18, 22, -3), (15, 19, 0), (10, 38, 7), (12, 19, 5), \\ &amp;\phantom{=} (17, 24, 0), (2, 30, 12), (3, 28, 11), (5, 33, 12)\,\}. \end{align*}\)</p> <p>One can quickly see that $\mathfrak P(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N) = \emptyset$, i.e., there are no adjacent dual points $p_1 + n_1,\,p_2 + n_2 \in \mathcal U^*(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)$ satisfying $p_1 - n_1 = p_2 - n_2$.</p> <div> This implies $\mathcal U^*(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)/\sim = \mathcal U^*(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)$, giving $8$ affine regions. This is confirmed by <a href="/blog/2025/dual-representation/#fig:example-affregs">this figure</a> from the previous post, which plots the tessellation induced by $\mathcal N$ and contains $8$ affine regions. </div> <div class="row mt-3 w-50 mx-auto figure-content" id="fig:tropical-toy-example-sum"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dual_representation/running-example-sum.svg" sizes="95vw"/> <img src="/assets/img/dual_representation/running-example-sum.svg" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption-left figure"> Two-dimensional toy-example continued. Purple points correspond to $P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N$. The purple polygon is $\mathcal U(P \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} N)$. </div>]]></content><author><name>Philipp Nazari</name></author><category term="Deep-Learning"/><category term="Geometry"/><category term="Generalization"/><summary type="html"><![CDATA[In this post, we introduce dual complexity measures for fully connected feedforward ReLU networks. This is part two of a three part series on the geometry of generalization of deep neural networks.]]></summary></entry><entry><title type="html">The Dual Representation of ReLU Networks</title><link href="https://phnazari.github.io/blog/2025/dual-representation/" rel="alternate" type="text/html" title="The Dual Representation of ReLU Networks"/><published>2025-06-02T00:00:00+00:00</published><updated>2025-06-02T00:00:00+00:00</updated><id>https://phnazari.github.io/blog/2025/dual-representation</id><content type="html" xml:base="https://phnazari.github.io/blog/2025/dual-representation/"><![CDATA[<h1 id="the-dual-representation">The Dual Representation</h1> <p>This is part one of a three part series on the geometry of generalization, which is the essence of my <a href="/assets/pdf/Master_Thesis.pdf">master thesis</a><d-cite key="nazari2025thesis"></d-cite>. In this series, we will present a novel perspective on generalization of overparameterized networks.</p> <p>The series is structured in the following way:</p> <ul> <li>In <a href="/blog/2025/dual-representation/">this post</a>, we establish a dual representation of fully connected feedforward ReLU networks.</li> <li>In <a href="/blog/2025/dual-complexity-measures/">part two</a>, we show how this dual representation can be used to derive complexity measures for these networks.</li> <li>In part three (coming soon!!), we will use these complexity measures to find evidence for the volume hypothesis<d-cite key="chiang2022loss"></d-cite>, an approach to explain why overparameterized models generalize well.</li> </ul> <h2 id="introduction">Introduction</h2> <p>The constructions put forward in this post are inspired by Piwek et al.<d-cite key="piwek2023exact"></d-cite>. In particular, we will establish a <em>dual representation</em> of fully connected feedforward ReLU networks, a symbolic representation that allows thinking about ReLU networks and their complexity in an abstract setting. This will be useful in part two and three of this series.</p> <p>The dual representation has been established in a number of previous works<d-cite key="piwek2023exact, zhang2018tropical, alfarra2022decision"></d-cite> and utilizes tropical geometry. While this is an interesting theory, one can use the close relationship between tropical geometry and affine geometry to make the constructions easier to interpret.</p> <p>However, throughout this series, I will refrain from proving all statements in detail. In particular, one can quickly forget about tropical geometry and its relationship to affine geometry. The curious reader may refer to Chapter 3 and Chapter 4 of my thesis<d-cite key="nazari2025thesis"></d-cite>, in particular Section 4.2, for more details on this relationship.</p> <p>Finally, for every claim made in this series, I will refer to the corresponding statement in the thesis. There, one can find a proof as well as a reference to the corresponding statement by Piwek et al.<d-cite key="piwek2023exact"></d-cite> wherever appropriate.</p> <h2 id="relu-networks">ReLU Networks</h2> <p>To introduce notation and get everybody on board, we start by quickly defining fully connected feedforward ReLU networks.</p> <div class="statement definition" id="def:nn"> <strong>(Fully Connected Feedforward Networks)</strong> A fully connected feedforward network $\mathcal N := \mathbb R^{d} \to \mathbb R^{n_L}$ takes as an input a vector $\mathbf x \in \mathbb R^{d}$ and returns an output $\mathbf y := \mathbf a_L$. It is defined inductively by $$ \left\{ \begin{aligned} \mathbf a_0 &amp;:= \mathbf x \\ \mathbf a_{l+1} &amp;= \rho_{t_{l+1}}\left({\mathbf W}_{l+1} \mathbf a_l + \mathbf b_{l+1}\right), \; 0 = 1,\ldots,L-1, \end{aligned} \right. $$ where $\mathbf W_{l+1} \in \mathbb R^{n_{l+1}, n_{l}}$ and $\mathbf b_{l+1} \in \mathbb R^{n_{l+1}}$ are the <em>weight matrix</em> and <em>bias vector</em> at layer $l+1$. Furthermore, $\rho_{t_{l+1}}(x) = \max(x, t_{l-1})$ is the <em>activation function</em> at layer $l+1$ with <em>threshold</em> $t_l \in \mathbb R \cup \{-\infty\}$. The number $L$ is called the <em>depth</em> of the network, while $n_l$ is the <em>width</em> of layer $l$. The network is <em>deep</em> if $L \gg 1$. </div> <p>The two activation functions we consider are:</p> <ul> <li> the ReLU $\rho_0(x) = \max(x,0)$</li> <li> the identity $\rho_{-\infty}(x) = x$.</li> </ul> <p>This leads to the following definition:</p> <div class="statement definition"> <strong>(ReLU Networks)</strong> A network in the sense of <a href="#def:nn" class="cite-stmt hover-link">Definition</a> with ReLU activations (and potentially a linear activation at the last layer) is called a <em>ReLU network</em>. </div> <h2 id="affine-geometry">Affine Geometry</h2> <p>In this section, we introduce fundamental concepts of affine geometry, covering basic definitions, the dual representation of affine functions, and their connection to upper convex hulls. Throughout this section, fix an integer $d \in \mathbb N$.</p> <h3 id="affine-and-cpa-functions">Affine and CPA Functions</h3> <p>We begin with some fundamental concepts.</p> <div class="statement definition" id="def:affine-functions"> <strong>(Affine Functions)</strong> Given a vector $\mathbf{a} \in \mathbb R^d$ and a scalar $b \in \mathbb R$, we define the affine function with parameters $\mathbf a$ and $b$ as $$ \begin{align*} f_{\mathbf a, b} \colon \mathbb R^d &amp;\to \mathbb R \\ \mathbf x &amp;\mapsto \langle \mathbf a, \mathbf x \rangle + b, \end{align*} $$ where $\langle \cdot, \cdot \rangle$ is the Euclidean inner product on $\mathbb R^d$. </div> <p>Ultimately, we will be taking maxima over affine maps. To classify such maxima, we introduce CPA functions:</p> <div class="statement definition"> <strong>(CPA Functions)</strong> We say that a function $f \colon \mathbb R^d \to \mathbb R$ is CPA if it is convex and piecewise affine. We denote by $\text{CPA}(d)$ the set of CPA functions $\mathbb R^d \to \mathbb R$. </div> <p>It turns out that the class of CPA functions coincides with the class of maxima over affine functions:</p> <div class="statement proposition" id="proposition:cpa"> <strong>(Characterizing CPA Functions, Proposition 2 in Piwek et al.<d-cite key="piwek2023exact"></d-cite>)</strong> Every function $F \colon \mathbb R^d \to \mathbb R$ of the form $$ \begin{equation*} F(\mathbf x) = \max\{f_1(\mathbf x),\ldots,f_n(\mathbf x)\} \end{equation*} $$ with affine functions $f_i \colon \mathbb R^d \to \mathbb R$ is CPA. Also every CPA function with a finite number of affine pieces is of this form. </div> <p>Later in this series, we will also consider differences of CPA functions:</p> <div class="statement definition"> <strong>(DCPA Functions)</strong> We say that a function $f \colon \mathbb R^d \to \mathbb R$ is DCPA if it can be written as the difference of two CPA functions. We denote by $\text{DCPA}(d)$ the set of DCPA function $\mathbb R^d \to \mathbb R$. </div> <h3 id="affine-dualities">Affine Dualities</h3> <p>In this section, we mainly follow the argument presented by Piwek et al.<d-cite key="piwek2023exact"></d-cite>, which allows mapping an affine function $f_{\mathbf a, b} \colon \mathbb R^d \to \mathbb R$ to a dual space. As an outlook, exploring this transformation will ultimately lead to understanding how ReLU networks can be understood as DCPA functions.</p> <div class="statement definition"> The graph of an affine function $\mathbb R^d \to \mathbb R$ defines a hyperplane in <em>real space</em>, which we define as $\mathcal R := \mathbb R^d \times \mathbb R = \mathbb R^{d+1}$. The space of affine functions whose graph lies in $\mathcal R$ is called <em>real affine space</em>, denoted by $\text{Aff}_{\mathfrak R}(d)$. </div> <p>As mentioned in <a href="#def:affine-functions" class="cite-stmt hover-link">Definition</a>, any affine function $f_{\mathbf{a},b} \in \text{Aff}_{\mathfrak{R}}(d)$ is characterized by its parameters $(\mathbf a, b) \in \mathbb R^{d+1}$:</p> <div class="statement definition"> We refer to the copy of $\mathbb R^{d+1}$ that parametrizes affine functions in $\text{Aff}_{\mathfrak{R}}(d)$ as <em>dual space</em>, denoted by $\mathcal D$. </div> <p>The following lemma is a natural consequence of this construction, as it allows translating between real affine space and dual space:</p> <div class="statement lemma"><strong>(Lemma 3.2.1 in Nazari<d-cite key="piwek2023exact"></d-cite>)</strong> For any fixed dimension $d$, there exists a bijection between dual space and real affine space, given by $$ \begin{align*} \mathcal R \colon \mathcal D &amp;\xrightarrow{\sim} \text{Aff}_{\mathfrak R}(d) \\ (\mathbf x, y) &amp;\mapsto f_{\mathbf x, y}. \end{align*} $$ </div> <div class="row mt-3 align-items-center" id="fig:ex-dual"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dual_representation/example_dual_point.svg" sizes="95vw"/> <img src="/assets/img/dual_representation/example_dual_point.svg" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"><strong>(a)</strong></figcaption> </figure> </div> <div class="caption-left"> </div> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dual_representation/example_dual_plane.svg" sizes="95vw"/> <img src="/assets/img/dual_representation/example_dual_plane.svg" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"><strong>(b)</strong></figcaption> </figure> </div> </div> <div class="caption-left figure"> Example of the dual representation of an affine map $f{\mathbf a, b}$ with $\mathbf a = (-1/2, -3/4)$, $b=3/4$. Subfigure <strong>(b)</strong> contains the graph of $f_{\mathbf a,b}$ and Subfigure <strong>(a)</strong> contains the parameterizing dual point $(\mathbf a, b) \in \mathcal D$. The map $\mathcal R$ assigns to the point $(\mathbf a,b)$ the affine map $f_{\mathbf a, b}$. </div> <p>An example for $\mathcal R$ can be found in <a href="#fig:ex-dual" class="cite-fig hover-link">Figure</a>. It has the following properties.</p> <div class="statement proposition"><strong>(Proposition 3.2.2 in Nazari<d-cite key="nazari2025thesis"></d-cite>)</strong> Let $\{\mathbf x_i, y_i\}_{i=1,\ldots,n} \subseteq \mathcal D$ be a set of dual points. Then the following are true: <ol type="i"> <li> $\mathcal R$ is a linear operator, i.e., for any set of scalars $\{\alpha_i\}_{i=1,\ldots,n} \subseteq \mathbb R$, $$ \begin{equation*} \mathcal R\left(\sum_{i=1}^n \alpha_i (\mathbf x_i, y_i)\right) = \sum_{i=1}^n \alpha_i \mathcal R((\mathbf x_i,y_i)). \end{equation*} $$ </li> <li> The set of dual points is linearly independent if and only if the corresponding set $\{\mathcal R((\mathbf x_i,y_i))\}_{i=1,\ldots,n}$ of affine functions is linearly independent. </li> <li> The set of dual points is affinely independent if and only if the corresponding set $\{\mathcal R((\mathbf x_i,y_i))\}_{i=1,\ldots,n}$ of affine functions is affinely independent. </li> </ol> </div> <p>Since both $\mathcal R$ and $\mathcal D$ are copies of $\mathbb R^{d+1}$, it is natural to ask whether we can reverse their roles in the above construction. The answer to this question is yes. We define <em>dual affine space</em> $\text{Aff}_{\mathfrak D}(d)$ as the space of affine functions with graph in $\mathcal D$. Analogously to the above construction, these affine functions are parameterized by points in $\mathcal R$, though with a slight caveat:</p> <div class="statement lemma"> For any fixed dimension $d$, there exists a bijection between dual affine space and real space. It is given by $$ \begin{align*} \check{\mathcal R} \colon \text{Aff}_{\mathfrak D}(d) &amp;\xrightarrow{\sim} \mathcal R \\ f_{\mathbf a, b} &amp;\mapsto (-\mathbf a, b). \end{align*} $$ </div> <div> <a href="#fig:real-dual-diagram" class="cite-fig hover-link">Figure</a> provides an overview over the relationship between $\mathcal R, \mathcal D, \text{Aff}_{\mathfrak R}(d)$ and $\text{Aff}_{\mathfrak D}(d)$. </div> <div class="row mt-3 w-50 mx-auto figure-content" id="fig:real-dual-diagram"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dual_representation/diagram.svg" sizes="95vw"/> <img src="/assets/img/dual_representation/diagram.svg" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption-left figure"> Diagram indicating the relationship between real (affine) and dual (affine) space. </div> <p>Note that, compared to $\mathcal R$, the function $\check{\mathcal R}$ includes an additional minus and maps in the opposite direction. This is essential for ensuring that the duality properties in the following proposition hold:</p> <div class="statement proposition"> <strong>(Duality Properties, Proposition 7 in Piwek et al.<d-cite key="piwek2023exact"></d-cite>) </strong> The maps $\mathcal R$ and $\check{\mathcal R}$ have the following properties: <ol style="i"> <li> A dual point $\mathbf c \in \mathfrak D$ lies on the graph of a dual affine function $f_{\mathbf a,b} \in \text{Aff}_{\mathfrak D}(d)$ if and only if the graph of the corresponding real affine function $\mathcal R(\mathbf c)$ contains the corresponding real point $\check{\mathcal R}(f_{\mathbf a,b})$. </li> <li> A dual point $\mathbf c \in \mathcal D$ lies above the graph of a dual affine function $f_{\mathbf a,b} \in \text{Aff}_{\mathfrak D}(d)$ if and only if the real point $\check{\mathcal R}(f_{\mathbf a,b})$ lies below the graph of $\mathcal R(c)$. </li> </ol> </div> <h3 id="cpa-functions-as-upper-convex-hulls">CPA Functions as Upper Convex Hulls</h3> <p>In the previous section, we explored a duality that allows identifying affine maps with the vector containing their parameters. In this section, we apply these results to maxima over affine functions, which, by <a href="#proposition:cpa" class="cite-stmt hover-link">Proposition</a>, can be understood as CPA functions.</p> <p>In light of the duality results from the previous section, CPA functions correspond to finite sets of dual points:</p> <div class="statement definition" id="def:q"> On the set $\mathcal P_\text{fin}(\mathcal D)$ of finite subsets of $\mathcal D$, the operator $$ \begin{align*} \mathcal Q \colon \mathcal P_\text{fin}(\mathcal D) &amp;\to \text{CPA}(d)\\ S &amp;\mapsto \mathcal Q(S) := \max_{\mathbf s \in S} \mathcal R(\mathbf s) \end{align*} $$ assigns to a set of dual points the associated CPA function $$ \begin{align*} \max_{\mathbf s \in S}\mathcal R(\mathbf s)(\mathbf x) = \max_{(\mathbf a, b) \in S} \langle \mathbf x, \mathbf a \rangle + b. \end{align*} $$ We define $\mathcal Q(\emptyset) := 0$. On a vector of finite sets of dual points, $Q$ acts component-wise. </div> <p>Note that, by <a href="#proposition:cpa" class="cite-stmt hover-link">Proposition</a>, the operator $\mathcal Q$ does indeed map to $\text{CPA}(d)$.</p> <div class="row mt-3 w-50 mx-auto figure-content" id="fig:example-uch"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dual_representation/example_uch.svg" sizes="95vw"/> <img src="/assets/img/dual_representation/example_uch.svg" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption-left figure"> Example of an upper convex hull. Let $S$ be the union of all displayed points. The blue points correspond to $\mathcal U^*(S)$. In particular, $\mathcal Q(S)$ is uniquely identified by those points. </div> <p>Our next objective is to establish a connection between CPA functions and upper convex hulls. To begin, we first state the following proposition:</p> <div class="statement proposition"> <strong>(Maximality of Upper Convex Hull, Proposition 3.3.2 in Nazari<d-cite key="nazari2025thesis"></d-cite>)</strong> Let $S \subseteq \mathcal D$ be a finite set of points. Then for every point $w \in \mathcal D$ lying below or on $\mathcal U(S)$, the affine function dual to $w$ lies fully below the maximum of the affine functions whose duals lie in $\mathcal U^*(S)$. That is, $$ \begin{equation} \mathcal R(w) \leq \max\{\mathcal R(s) | s \in \mathcal U^*(S)\} = \mathcal Q(\mathcal U^*(S)). \end{equation} $$ If $w$ lies truly below $\mathcal U(S)$, then even $$ \begin{equation} \mathcal R(w) &lt; \mathcal Q(\mathcal U^*(S)). \end{equation} $$ </div> <p>Having established this proposition, the identification of CPA functions with upper convex hulls is a corollary:</p> <div class="statement corollary" id="corollary:upper-convex-hull-rep"> <strong>(CPAs as Upper Convex Hulls, Corollary 3.3.3 in Nazari<d-cite key="nazari2025thesis"></d-cite>)</strong> Every CPA function $\mathcal Q(S)$ can be uniquely represented as an upper convex hull in dual space. That is, $\mathcal Q(S) = \mathcal Q(\mathcal U^*(S))$. </div> <p>A visualization of <a href="#corollary:upper-convex-hull-rep" class="cite-stmt hover-link">Corollary</a> can be found in <a href="#fig:example-uch" class="cite-fig hover-link">Figure</a>.</p> <h2 id="dual-representation-of-neural-networks">Dual Representation of Neural Networks</h2> <p>Using the above constructions, we will now establish a connection between fully connected feedforward networks and DCPA functions. This will ultimately enable us to translate the network to dual space.</p> <h3 id="neural-networks-and-affine-geometry">Neural Networks and Affine Geometry</h3> <p>In order to establish the connection, we first need to develop some more machinery, beginning with the definition of how to sum two sets:</p> <div class="statement definition"> Given two non-empty sets $X, Y \subseteq \mathbb R^{d+1}$, we define $$ \begin{equation*} X \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} Y := \{\mathbf x + \mathbf y | \mathbf x \in X, \mathbf y \in Y\} \end{equation*} $$ to be the <em>Minkowski sum</em> of $X$ and $Y$. We define $X \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} \emptyset := X$. On vectors of sets of dual points, we define $\style{transform: rotate(45deg); display: inline-block;}{\boxtimes}$ to act component-wise. </div> <p>Next, we list some properties of the operator $\mathcal Q$ (see <a href="#def:q" class="cite-stmt hover-link">Definition</a>), which assigns to a set of dual points the corresponding CPA function:</p> <div class="statement lemma"> <strong>(Properties of $\mathcal Q$, Lemma 5.1.2 in <d-cite key="nazari2025thesis"></d-cite>)</strong> For any two sets of points $X, Y \subseteq \mathcal D$ and every non-negative scalar $\alpha \geq 0$, the following are true: <ol style="i"> <li> $\mathcal Q(X \cup Y) = \max\{\mathcal Q(X), \mathcal Q(Y)\}$ </li> <li> $\mathcal Q(X \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} Y) = \mathcal Q(X) + \mathcal Q(Y)$ </li> <li> $\alpha \cdot \mathcal Q = \mathcal Q(\alpha \cdot X)$, where the multiplication on the right hand side is the natural multiplication of a set with a real number. </li> </ol> </div> <p>Neural networks rely heavily on matrix multiplications. On our mission to translate them to dual space, we must establish the concept of matrix multiplication in the dual setting:</p> <div class="statement definition"> We define the multiplication of an $m \times n$ matrix $\mathbf A$ with a vector $X$ of $n$ finite sets of dual points as $$ \begin{align*} \cdot \colon \mathbb R^{m, n} \times \left(P_\text{fin}(\mathcal D)\right)^n &amp;\to \left(P_\text{fin}(\mathcal D)\right)^m\\ (\mathbf A, X) &amp;\mapsto \mathbf A \cdot X \end{align*} $$ where $$ (\mathbf A \cdot X)_i := {\style{transform: rotate(45deg); display: inline-block; font-size: 150%;}{\boxtimes}}_{j=1}^{n} \mathbf A_{ij} \cdot X_j \quad \forall i=1,\ldots,m. $$ In the notation above, $\left(P_\text{fin}(\mathcal D)\right)^n$ denotes the $n$-fold Cartesian product of $P_\text{fin}(\mathcal D)$ with itself and ${\style{transform: rotate(45deg); display: inline-block; font-size: 150%;}{\boxtimes}}_{j=1}^n$ denotes the Minkowski sum over the sets indexed by $\{1,\ldots,n\}$. </div> <p>The following lemma shows that matrix multiplication and the $\mathcal Q$-operator commute:</p> <div class="statement lemma"> <strong>(Matrix Multiplication, Lemma 5.1.4 in Nazari<d-cite key="nazari2025thesis"></d-cite>) </strong> Let $X \in \left(P_\text{fin}(\mathcal D)\right)^n$ be a vector of finite sets of dual points and $\mathbf A \in \mathbb R_+^{m, n}$ a matrix with non-negative entries. Then $$ \begin{equation*} \mathbf A \mathcal Q(X) = \mathcal Q(\mathbf A \cdot X). \end{equation*} $$ </div> <p>In order to account for biases, we define how to add a scalar to a set of dual points:</p> <div class="statement definition"> A scalar can be added to a set of dual points by adding the scalar to the last entry of each point in the set: $$ \begin{align*} \boxplus \colon P_\text{fin}(\mathcal D) \times \mathbb R &amp;\to P_\text{fin}(\mathcal D) \\ (X, \alpha) &amp;\mapsto X \boxplus \alpha, \end{align*} $$ where $X \boxplus \alpha$ is the set $$ \begin{equation*} X \boxplus \alpha := \{(\mathbf x, y + \alpha) | (\mathbf x, y) \in X\}. \end{equation*} $$ </div> <p>It turns out that $\mathcal Q$ is also well behaved with respect to $\boxplus$:</p> <div class="statement lemma"> <strong>(Lemma 5.1.6 in Nazari<d-cite key="nazari2025thesis"></d-cite>)</strong> For any finite set of dual points $X \subseteq \mathcal D$ and scalar $\alpha \in \mathbb R$, it holds that $$ \begin{equation*} \mathcal Q(X) + \alpha = \mathcal Q(X \boxplus \alpha). \end{equation*} $$ </div> <p>We are now ready to present the following fundamental proposition that establishes the connection between ReLU networks and differences of piecewise affine functions:</p> <div class="statement proposition" id="proposition:nn-as-affine-map"> <strong>(Dual Representation, Proposition 5.1.7 in <d-cite key="nazari2025thesis"></d-cite>) </strong> Assume that a neural network in the sense of <a href="#def:nn" class="cite-stmt hover-link">Definition</a> can, up to layer $l-1$, be written as a DCPA function ${\mathbf a}_{l-1} = \mathcal Q(P_{l-1})- \mathcal Q(N_{l-1})$ for some vectors of finite sets of dual points $P_{l-1}, N_{l-1}$. Then, after writing ${\mathbf W}_l = {\mathbf W}_l^+ - {\mathbf W}_l^-$ using matrices ${\mathbf W}_l^+$ and ${\mathbf W}_l^-$ with non-negative entries, also the network up to the $l$'th layer can be written as a DCPA function $$ \begin{equation*} {\mathbf a}_l = \mathcal Q(P_l) - \mathcal Q(N_l) \end{equation*} $$ with $$ \begin{align*} N_l &amp;= ({\mathbf W}_l^- \cdot P_{l-1}) \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} ({\mathbf W}_l^+ \cdot N_{l-1}) \\ P_l &amp;= \left( \left(\left({\mathbf W}_l^+ \cdot P_{l-1}) \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} ({\mathbf W}_l^- \cdot N_{l-1}\right)\right) \boxplus {\mathbf b}_l \right) \cup \begin{cases} N_l \boxplus t_l, &amp; t_l \neq - \infty \\ \emptyset, &amp; t_l = -\infty. \end{cases} \end{align*} $$ </div> <p>The following corollary makes sure <a href="#proposition:nn-as-affine-map" class="cite-stmt hover-link">Proposition</a> can actually be applied to ReLU networks by establishing the base case:</p> <div class="statement corollary" id="corollary:dual-rep"> Every neural network $\mathcal N$ in the sense of <a href="#def:nn" class="cite-stmt hover-link">Definition</a> can be written as a DCPA function $$ \begin{equation*} \mathcal N = \mathcal Q(P) - \mathcal Q(N) \end{equation*} $$ for some vectors of sets of dual points $P, N \subseteq \mathcal D$. We call $(P, N)$ the <em>dual representation</em> of $\mathcal N$. </div> <p>The dual representation is denoted by $(P, N)$ since those two sets tell us which points are classified positively and negatively:</p> <div class="statement proposition"> <strong>(Positive and negative samples, Proposition 5.1.11 in Nazari<d-cite key="nazari2025thesis"></d-cite>)</strong> Let $\mathcal N = \mathcal Q(P) - \mathcal Q(N)$ be a ReLU binary classification network. Then the following are true for an input $\mathbf x \in \mathbb R^d$: $$ \begin{align} \mathcal N(\mathbf x) \geq 0 &amp;\iff \mathcal Q(P \cup N)(\mathbf x) = \mathcal Q(P)(\mathbf x) \\ \mathcal N(\mathbf x) \leq 0 &amp;\iff \mathcal Q(P \cup N)(\mathbf x) = \mathcal Q(N)(\mathbf x). \end{align} $$ </div> <p>The following remark summarizes the dual representation and why it is relevant.</p> <div class="statement remark"> <a href="#proposition:nn-as-affine-map" class="cite-stmt hover-link">Proposition</a> and <a href="#corollary:dual-rep" class="cite-stmt hover-link">Corollary</a> are important tools throughout the rest of this series. We want to use this remark to highlight their significance. Let $\mathcal N \colon \mathbb R^d \to \mathbb R$ be a ReLU network with $L$ layers. <ol> <li> Given the weights of $\mathcal N$, the dual representation provides a symbolic representation $(P_l, N_l)$ of $\mathcal N$ up to layer $l$. </li> <li> It is given by two $n_l$-dimensional vectors $P_l, N_l$ of finite sets of dual points, where $n_l$ is the width of layer $l$. The dual points are $d+1$-dimensional. </li> <li> After each layer $l$, the sets of dual points can be replaced by their upper convex hull (see <a href="#corollary:upper-convex-hull-rep" class="cite-stmt hover-link">Corollary</a>). In particular, for every $i \in \{1,\ldots,n_l\}$, the set $(P_l)_i \subseteq \mathcal D = \mathbb R^{d+1}$ can be replaced by its upper convex hull vertices $\mathcal U^*((P_l)_i)$. The same holds for $(N_l)_i$. </li> <li> As we will see later, this symbolic representation allows counting the number of affine regions defined by $\mathcal N$. In the case of binary classification, it furthermore allows counting the linear pieces in the decision boundary (Post 2). </li> </ol> </div> <p>Scattered throughout this series, we employ a running example to highlight the above points.</p> <h3 id="example">Example</h3> <p>In this example, we construct the dual representation of a toy example in two dimensions (see Example 5.2.6 in Nazari<d-cite key="nazari2025thesis"></d-cite>). Throughout this series, we will revisit and use this example to explain various aspect of the discussed duality result.</p> <div class="row mt-3" id="fig:example-affregs"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dual_representation/example_linear_regions-480.webp 480w,/assets/img/dual_representation/example_linear_regions-800.webp 800w,/assets/img/dual_representation/example_linear_regions-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dual_representation/example_linear_regions.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"><strong>(a)</strong></figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dual_representation/example_decision_boundary-480.webp 480w,/assets/img/dual_representation/example_decision_boundary-800.webp 800w,/assets/img/dual_representation/example_decision_boundary-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dual_representation/example_decision_boundary.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"><strong>(b)</strong></figcaption> </figure> </div> </div> <div class="caption-left figure"> Subfigure <strong>(a)</strong> shows the affine regions defined by the network defined in Equation \eqref{eq:tropical-toy-example}. Subfigure <strong>(b)</strong> shows its decision boundary. Negatively classified regions (threshold at $0$) are colored red and positively classified regions are colored blue. </div> <p>Specifically, consider the $3$ layer network</p> \[\begin{align} \label{eq:tropical-toy-example} \mathcal N \colon \mathbb R^2 &amp;\to \mathbb R \\ {\mathcal N}(x) &amp;= {\mathbf W}_3\rho_0\left({\mathbf W}_2\rho_0\left({\mathbf W}_1 \mathbf x + {\mathbf b}_1\right) + {\mathbf b}_2\right) + b_3 \end{align}\] <p>where</p> \[\begin{align*} {\mathbf W}_1 &amp;= \begin{pmatrix} -1 &amp; -1 \\ 1 &amp; -2 \end{pmatrix}, {\mathbf b}_1 = \begin{pmatrix} 1 \\ -1 \end{pmatrix}, {\mathbf W}_2 = \begin{pmatrix} -1 &amp; 2 \\ 2 &amp; -1 \end{pmatrix}, {\mathbf b}_2 = \begin{pmatrix} 1 \\ 2 \end{pmatrix}, {\mathbf W}_3 = \begin{pmatrix} 3, -1 \end{pmatrix}, b_3 = 2. \end{align*}\] <p>We use <a href="#proposition:nn-as-affine-map" class="cite-stmt hover-link">Proposition</a> to iteratively construct the dual representation of $\mathcal N$, starting with $P_0 = ({(1,0,0)}, {(0,1,0)})$ and $N_0 = (\emptyset)$, as in the proof of <a href="#corollary:dual-rep" class="cite-stmt hover-link">Corollary</a>.</p> <p>After the first layer, the dual representation of $N_1$ can be computed as</p> \[\begin{align*} N_1 &amp;= {\mathbf W}_1^- P_0 \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} {\mathbf W}_1^+ N_0 = \begin{pmatrix} 1 &amp; 1 \\ 0 &amp; 2 \end{pmatrix} P_0 \end{align*}\] <p>and thus</p> \[\begin{align*} (N_1)_1 &amp;= {\style{transform: rotate(45deg); display: inline-block; font-size: 150%;}{\boxtimes}}_{j=1}^2 ({\mathbf W}_1^-)_{1j} (P_0)_j = 1\left\{\left((1,0,0)\right)\right\} \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} 1\left\{\left(0,1,0\right)\right\} = \left\{\left(1,1,0\right)\right\} \\ (N_1)_2 &amp;= {\style{transform: rotate(45deg); display: inline-block; font-size: 150%;}{\boxtimes}}_{j=1}^2 ({\mathbf W}_1^-)_{2j} (P_0)_j = 0\left\{\left((1,0,0)\right)\right\} \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} 2\left\{\left(0,1,0\right)\right\} = \left\{\left(0,2,0\right)\right\}, \end{align*}\] <p>which implies</p> \[\begin{equation*} N_1 = \left(\left\{\left(1,1,0\right)\right\}, \left\{\left(0,2,0\right)\right\}\right). \end{equation*}\] <p>Similarly,</p> \[\begin{align*} P_1 &amp;= {\mathbf W}_1^+ P_0 \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} {\mathbf W}_1^- N_0 \boxplus {\mathbf b}_1 \cup N_1 = \begin{pmatrix} 0 &amp; 0 \\ 1 &amp; 0 \end{pmatrix} P_0 \boxplus {\mathbf b}_1 \cup N_1 \end{align*}\] <p>and thus</p> \[\begin{align*} (P_1)_1 &amp;= {\style{transform: rotate(45deg); display: inline-block; font-size: 150%;}{\boxtimes}}_{j=1}^{2} ({\mathbf W}_1^+)_{1j} (P_0)_j \boxplus ({\mathbf b}_1)_1 \cup (N_1)_1 = 0\left\{\left((1,0,0)\right)\right\} \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} 0\left\{\left(0,1,0\right)\right\} \boxplus 1 \cup (N_1)_1 \\ &amp;= \left\{\left(0,0,1\right), \left(1,1,0\right)\right\} \\ (P_1)_2 &amp;= {\style{transform: rotate(45deg); display: inline-block; font-size: 150%;}{\boxtimes}}_{j=1}^{2} ({\mathbf W}_1^+)_{2j} (P_0)_j \boxplus ({\mathbf b}_1)_2 \cup (N_1)_2 = 1\left\{\left((1,0,-1)\right)\right\} \style{transform: rotate(45deg); display: inline-block;}{\boxtimes} 0\left\{\left(0,1,0\right)\right\} \boxplus -1 \cup (N_1)_2 \\ &amp;= \left\{\left(1,0,0\right), \left(0, 2, 0\right)\right\}, \end{align*}\] <p>which implies</p> \[\begin{equation*} P_1 = \left(\left\{\left(0,0,1\right),\left(1,1,0\right)\right\}, \left\{\left(0,2,0\right),\left(1,0,-1\right)\right\}\right). \end{equation*}\] <p>After repeating these steps for layer $2$ and $3$ (with a slight adaptation for the last linear layer as in <a href="#proposition:nn-as-affine-map" class="cite-stmt hover-link">Proposition</a>), one arrives at the following final dual representation of $\mathcal N = \mathcal Q(P) - \mathcal Q(N)$:</p> \[\begin{align*} N &amp;= \left\{(3, 17, 4), (2, 16, 5), (5, 19, 2), (3, 14,2), (2,16,3),(5,19,0),(6,17,-1),(0,14,7)\right\} \\ P &amp;= \left\{(2,16,5),(5,19,2),(5,19,5),(11,7,-1),(12,5,-2),(3,14,4),(6,17,1),(6,17,4)\right\}. \end{align*}\] <p>Here, $N := N_3$ and $P := P_3$. Additionally, note that we have identified the one-dimensional vectors of sets $N_3$ and $P_3$ with their only entry.</p> <p>By <a href="#corollary:upper-convex-hull-rep" class="cite-stmt hover-link">Corollary</a>, the CPA functions $\mathcal Q(N)$ and $\mathcal Q(P)$ are uniquely identified by the upper convex hulls of $N$ and $P$, i.e.,</p> \[\begin{equation*} \mathcal Q(N) = \mathcal Q(\mathcal U^*(N)), \quad \mathcal Q(P) = \mathcal Q(\mathcal U^*(P)). \end{equation*}\] <p>This allows restricting our attention to subsets of $N$ and $P$. Specifically, the upper convex hull points can be determined\footnote{We use SciPy to do so, see the code for more details.} as</p> \[\begin{align*} \mathcal U^*(N) &amp;= \left\{(5, 19, 2), (3, 14, 2), (6, 17, -1), (0, 14, 7)\right\} \\ \mathcal U^*(P) &amp;= \left\{(2,16,5), (3,14,4), (5,19,5), (12,5,-2)\right\}. \end{align*}\] <p><a href="#fig:tropical-toy-example" class="cite-fig hover-link">Figure</a> contains a plot of the dual representation of this toy example, as well as the upper convex hulls.</p> <div class="row mt-3 w-50 mx-auto figure-content" id="fig:tropical-toy-example"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dual_representation/example_dual.svg" sizes="95vw"/> <img src="/assets/img/dual_representation/example_dual.svg" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption-left figure"> Dual representation of the two-dimensional toy-example defined in Equation \eqref{eq:tropical-toy-example}. Red points correspond to $N$, blue points are $P$. The red polygon is $\mathcal U(N)$, the blue polygon is $\mathcal U(P)$. Note that, in theory, both $\mathcal U(N)$ and $\mathcal U(P)$ are polyhedral complexes, i.e., they can consist of multiple facets. </div> ]]></content><author><name>Philipp Nazari</name></author><category term="Deep-Learning"/><category term="Geometry"/><category term="Generalization"/><summary type="html"><![CDATA[In this post, we introduce the dual representation of fully connected feedforward ReLU networks. This is part one of a three part series on the geometry of generalization of deep neural networks.]]></summary></entry></feed>